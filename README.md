# vLLM Qwen3-Coder-30B na RunPod (A40 Optimized)

Setup vLLM z modelem Qwen3-Coder-30B zoptymalizowany pod A40 48GB z FP8 dla maksymalnej wydajnoÅ›ci.

## Zalety vs GGUF/Ollama

- âš¡ **5-10x szybszy inference** dziÄ™ki PagedAttention
- ðŸ“Š **Continuous batching** - obsÅ‚uga wielu requestÃ³w jednoczeÅ›nie  
- ðŸ”Œ **OpenAI-compatible API** - Å‚atwa integracja
- ðŸ’¾ **Efektywne zarzÄ…dzanie pamiÄ™ciÄ…** - lepsze wykorzystanie VRAM
- ðŸŽ¯ **Natywna obsÅ‚uga dÅ‚ugich kontekstÃ³w** - do 128K tokenÃ³w

## Optymalizacja dla A40 48GB

- **GPU**: A40 48GB VRAM âœ…
- **RAM**: 48GB âœ…
- **CPU**: 9 vCPU âœ…
- **Dysk**: 60GB+ wolnego miejsca
- **RunPod**: Expose port 8000

## Konfiguracja A40 Optimized

- **Model**: BFloat16 + FP8 KV Cache
- **Kontekst**: 250K tokenÃ³w (wykorzystuje dodatkowÄ… VRAM!)
- **VRAM uÅ¼ycie**: ~35GB z 48GB (73% utilization)
- **Concurrent sequences**: 64 (dostosowane do 9 vCPU)
- **Performance**: 50-80 tokens/s, 8-16 concurrent users

## Automatyczny setup

```bash
# Klonuj/skopiuj pliki na RunPod
cd /workspace
git clone [YOUR_REPO] vllm-qwen-simple  # lub skopiuj przez SCP

cd vllm-qwen-simple

# Uruchom automatyczny setup
chmod +x setup.sh
sudo ./setup.sh
```

## RÄ™czne uruchomienie

```bash
# Zainstaluj Docker (jeÅ›li nie ma)
curl -fsSL https://get.docker.com | sh

# Uruchom vLLM
docker compose up -d

# SprawdÅº status
docker compose logs -f
```

## Testowanie API

### Health Check
```bash
curl http://localhost:8000/health
```

### Lista modeli
```bash
curl http://localhost:8000/v1/models
```

### Completion
```bash
curl http://localhost:8000/v1/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder",
    "prompt": "def fibonacci(n):",
    "max_tokens": 200,
    "temperature": 0.3
  }'
```

### Chat Completion
```bash
curl http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "qwen3-coder", 
    "messages": [
      {"role": "system", "content": "You are a helpful coding assistant."},
      {"role": "user", "content": "Write a Python web scraper using requests"}
    ],
    "max_tokens": 1000,
    "temperature": 0.7
  }'
```

## Python Client

```python
from openai import OpenAI

# Konfiguracja (uÅ¼yj swojego RunPod IP)
client = OpenAI(
    base_url="http://localhost:8000/v1",  # lub http://RUNPOD_IP:8000/v1
    api_key="token-abc123"  # Dowolny string
)

# Chat completion
response = client.chat.completions.create(
    model="qwen3-coder",
    messages=[
        {"role": "system", "content": "You are an expert Python developer."},
        {"role": "user", "content": "Create a FastAPI app with authentication"}
    ],
    max_tokens=2000,
    temperature=0.7,
    top_p=0.9
)

print(response.choices[0].message.content)

# Streaming
for chunk in client.chat.completions.create(
    model="qwen3-coder",
    messages=[{"role": "user", "content": "Explain async/await in Python"}],
    stream=True,
    max_tokens=1000
):
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

## Konfiguracja dla rÃ³Å¼nych GPU

### RTX 4090 (24GB) - FP8
```yaml
--max-model-len 131072          # 128K kontekst moÅ¼liwy z FP8!
--gpu-memory-utilization 0.90   # Bezpieczne dla 24GB
--kv-cache-dtype fp8_e4m3       # FP8 cache
```

### A40 48GB - FP8 (DOMYÅšLNA KONFIGURACJA)
```yaml
--max-model-len 250000          # 250K kontekst - wykorzystuje dodatkowÄ… VRAM
--gpu-memory-utilization 0.92   # Bezpieczne dla 48GB
--max-num-seqs 64               # Dostosowane do 9 vCPU
--kv-cache-dtype fp8_e4m3       # FP8 cache
--block-size 16                 # Optymalizacja dla A40
```

### A100 40GB - FP8
```yaml
--max-model-len 200000          # 200K kontekst z FP8
--gpu-memory-utilization 0.95   # Prawie peÅ‚ne wykorzystanie
--max-num-seqs 128              # WiÄ™cej CPU dostÄ™pne
--kv-cache-dtype fp8_e4m3
```

### A100 80GB / H100 - FP8
```yaml
--max-model-len 256000          # Maksymalny natywny kontekst Qwen3
--gpu-memory-utilization 0.95   # PeÅ‚ne wykorzystanie
--kv-cache-dtype fp8_e4m3       # Najlepsza wydajnoÅ›Ä‡
```

### Multi-GPU
```yaml
--tensor-parallel-size 2        # Dla 2 GPU
--pipeline-parallel-size 1
```

## Monitoring

```bash
# Status kontenerÃ³w
docker compose ps

# Logi vLLM
docker compose logs -f vllm

# GPU usage
nvidia-smi

# API metrics
curl http://localhost:8000/metrics
```

## Optymalizacja wydajnoÅ›ci

### 1. Adjust batch size
```yaml
--max-num-seqs 128              # WiÄ™cej rÃ³wnolegÅ‚ych requestÃ³w
```

### 2. Memory optimization
```yaml
--swap-space 32                 # WiÄ™cej CPU swap space
--gpu-memory-utilization 0.95   # Maksymalne uÅ¼ycie GPU
```

### 3. Attention backend
```yaml
environment:
  - VLLM_ATTENTION_BACKEND=FLASHINFER  # Najszybszy backend
```

## PorÃ³wnanie wydajnoÅ›ci

| Model Format | Engine | Speed | VRAM | Context | A40 Fit? |
|-------------|--------|-------|------|---------|----------|
| GGUF Q5_K_XL | Ollama | 1x | ~30GB | 128K | âœ… |
| FP16 | vLLM | 5-10x | ~45GB | 128K | âœ… |
| **FP8 A40 Opt** | **vLLM** | **8-15x** | **~35GB** | **250K** | **âœ… Perfect!** |

**A40 + FP8 = idealne poÅ‚Ä…czenie wydajnoÅ›Ä‡/pamiÄ™Ä‡/kontekst!**

## Troubleshooting

### OOM Error
```bash
# Zmniejsz parametry w .env:
MAX_MODEL_LEN=65536
GPU_MEMORY_UTILIZATION=0.85

# Restart
docker compose down && docker compose up -d
```

### Wolne uruchamianie
```bash
# Model pobiera siÄ™ przy pierwszym starcie (~30GB)
# SprawdÅº postÄ™p:
docker compose logs -f vllm
```

### API nie odpowiada
```bash
# SprawdÅº czy kontener dziaÅ‚a
docker compose ps

# SprawdÅº porty
netstat -tlnp | grep 8000

# Test health
curl -I http://localhost:8000/health
```

### Brak GPU
```bash
# Test NVIDIA runtime
docker run --rm --gpus all nvidia/cuda:12.0-base nvidia-smi

# JeÅ›li nie dziaÅ‚a - reinstaluj NVIDIA Container Toolkit
```

## Endpoints RunPod

Po expose port 8000 w RunPod:

- **Lokalny**: `http://localhost:8000`
- **Publiczny**: `http://[RUNPOD_IP]:8000` 
- **Proxy**: `https://[POD_ID]-8000.proxy.runpod.net`

## Model Information

- **Nazwa**: unsloth/Qwen3-Coder-30B-A3B-Instruct
- **Rozmiar**: ~30B parametrÃ³w  
- **Precyzja**: BFloat16 + FP8 KV Cache
- **Kontekst**: 250K tokenÃ³w (A40 optimized), do 256K max
- **Specjalizacja**: Kodowanie, programowanie
- **JÄ™zyki**: Python, JavaScript, Java, C++, Go, Rust, i inne

To znacznie prostszy i wydajniejszy setup niÅ¼ Ollama z GGUF!